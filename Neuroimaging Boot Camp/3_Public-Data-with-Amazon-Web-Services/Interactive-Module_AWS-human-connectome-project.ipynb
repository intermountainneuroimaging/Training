{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Amazon Web Services for Large Dataset Processing\n",
    "\n",
    "This is an interactive tutorial for the use of Amazon Web Services command line interface. In this tutorial we will locate and locally store a subset of neuroimaging files from the Human Connectome Project using Amazon S3 Public Access. All computations will be run using University of Coloraodo at Boulder Reserach Computing compute cluster. If you do not have access to CURC please follow the steps [here](https://rcamp.rc.colorado.edu/accounts/account-request/create/organization) to get started.\n",
    "\n",
    "**Learning Objectives:**\n",
    "- Use Amazon Web Services Command Line tools to explore the open-access datasets\n",
    "- Retrieve key information about the public dataset\n",
    "- Use aws to store a local copy of public data for analysis\n",
    "\n",
    "Using a command line tool to retrieve open-access data allows the user to store only the input data needed for current computations, and to remove those data when the local analysis is complete. This can significantly cut down on the static space needed to store these publically accessible datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Human Connectome Project Neuroimaging Data\n",
    "Before we get started, each user will need to generate an account on the human connectome project website, agree to the data license terms, and generate a amazon web services key.\n",
    "1. Navigate to https://db.humanconnectome.org/\n",
    "2. Create a free user account\n",
    "3. Locate the section: WU-Minn HCP Data - 1200 Subjects\n",
    "4. Click on data use terms and follow the instructiosns\n",
    "5. Open the Amazon S3 Access Window, and record both the access Key and Secret Key for use later.\n",
    "\n",
    "![database screen for human connectome project page](pics/human-connectome-project-img1.png)\n",
    "\n",
    "*Enter your access key information below:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export AWS_ACCESS_KEY_ID=<key-id>               #AKIAXO65CT57BKPAKPG3\n",
    "export AWS_SECRET_ACCESS_KEY=<secret-key-id>    #vUKqlL8n4kLWMKMVEejCatRfHQ967iCymJDKsfv0\n",
    "export AWS_DEFAULT_REGION=us-west-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# understand structure of Blanca Scratch strucutre and permissions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access Human Connectome Public Dataset on AWS S3 server\n",
    "Now that we have all necessary creidentials to work with the human connectome dataset, we can use aws command line utilitles reivew all availible data and to download and manipulate files of interest. First, lets add aws command line tools to our path and check whether it is installed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module use /projects/ics/modules\n",
    "module load aws\n",
    "aws --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Next lets use aws S3 servers to locate and select files from the human connectome project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the first 10 subjects stored in hcp dataset\n",
    "aws s3 ls s3://hcp-openaccess/HCP/ | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all the unprocessed images for an example subject\n",
    "aws s3 ls s3://hcp-openaccess/HCP/100307/unprocessed/3T/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q1:** How many subjects are stored in the HCP open access dataset? Enter your code below.\n",
    "*Hint:* use `wc -l` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q2:** What processed files are sorted in the HCP open access dataset? Enter your code below.\n",
    "*Hint:* Preprocessed subject data is located in `MNINonLinear` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Q3:** Search for other [open access](https://aws.amazon.com/opendata) datasets you may be insterested in exploring from aws s3. Point to a different dataset and explore the exisiting data files.  Enter your code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Analysis using Human Connectome Dataset\n",
    "We can use the human connectome public dataset for any computations run on the CU cluster (Blanca, Summit). To do this, first we download a local copy of the input data from hcp-openaccess. We will download these data to a scratch directory as we do not need to retain these images in perpetuity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj=100307   # subject_id_of_interest\n",
    "\n",
    "# remember if you want to only work with part of the dataset, \n",
    "#    you can indicate that by altering these prospective paths\n",
    "mkdir -p $SLURM_SCRATCH/$USER/HCP/${subj}/unprocessed/3T/T1w_MPR1\n",
    "aws s3 cp s3://hcp-openaccess/HCP/${subj}/unprocessed/3T/T1w_MPR1 \\\n",
    "               $SLURM_SCRATCH/$USER/HCP/${subj}/unprocessed/3T/T1w_MPR1 --recursive --quiet \n",
    "\n",
    "ls -l $SLURM_SCRATCH/$USER/HCP/${subj}/*/*/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add sync command to ignore existing commands (test this functionality)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An additional step you may choose to take is to convert the human connectome uprocessed (raw) images to BIDS compatible format. This allow you to use pipelines dedicated to preprocessing BIDS datasets such as `MRIQC` or `fMRIPrep`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $SLURM_SCRATCH/$USER\n",
    "mkdir -p $SLURM_SCRATCH/$USER/HCP_BIDS\n",
    "\n",
    "module load python/3.6.5\n",
    "\n",
    "# download hcp2bids tool if not present...\n",
    "if ! [ -d hcp2bids ]; then git clone https://github.com/niniko1997/hcp2bids.git ; fi\n",
    "cd hcp2bids/hcp2bids\n",
    "\n",
    "#convert hcp subjects to bids compatible\n",
    "python main.py $SLURM_SCRATCH/$USER/HCP $SLURM_SCRATCH/$USER/HCP_BIDS --symlink > hcp2bids.log\n",
    "\n",
    "# note: some bids tools require extra files not created from this function:\n",
    "#     T1w.json, T2w.json, dataset_description.json, .bidsignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls $SLURM_SCRATCH/$USER/HCP_BIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, its important to dump all local files when you are done using them\n",
    "rm -R $SLURM_SCRATCH/$USER/HCP\n",
    "rm -R $SLURM_SCRATCH/$USER/HCP_BIDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
